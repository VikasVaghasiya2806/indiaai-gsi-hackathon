{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11784258,"sourceType":"datasetVersion","datasetId":7398767}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler, PowerTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.manifold import TSNE\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nimport shap\nimport json\nfrom prettytable import PrettyTable\nimport plotly.graph_objects as go\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T14:02:24.023010Z","iopub.execute_input":"2025-05-12T14:02:24.023457Z","iopub.status.idle":"2025-05-12T14:02:24.029847Z","shell.execute_reply.started":"2025-05-12T14:02:24.023414Z","shell.execute_reply":"2025-05-12T14:02:24.028880Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# -------------------------\n# 1. DATA LOADING & CLEANING\n# -------------------------\ndef load_and_clean(file_path):\n    df = pd.read_excel(file_path)\n    df = df.drop(columns=['FID', 'Shape *', 'gid', 'objectid', 'toposheet'], errors='ignore')\n    essential_elements = ['X', 'Y', 'Si02_%', 'Al2O3_%', 'Fe2O3_%', 'TiO2_%',\n                        'MgO_%', 'MnO_%', 'Na2O_%', 'K2O_%', 'Ni_ppm', 'Co_ppm',\n                        'Cu_ppm', 'Zn_ppm', 'Au_ppb', 'La_ppm', 'Ce_ppm', 'Zr_ppm',\n                        'Hf_ppm', 'Pt_ppb', 'Pd_ppb']\n    df = df[essential_elements].dropna()\n    iso = IsolationForest(contamination=0.05, random_state=42)\n    outliers = iso.fit_predict(df.select_dtypes(include=np.number))\n    df = df[outliers == 1]\n    pt = PowerTransformer()\n    num_cols = df.select_dtypes(include=np.number).columns\n    df[num_cols] = pt.fit_transform(df[num_cols])\n    print(f\"âœ… Cleaned Data: {df.shape[0]} samples, {df.shape[1]} features\")\n    return df, pt\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T14:02:26.533827Z","iopub.execute_input":"2025-05-12T14:02:26.534178Z","iopub.status.idle":"2025-05-12T14:02:26.541790Z","shell.execute_reply.started":"2025-05-12T14:02:26.534151Z","shell.execute_reply":"2025-05-12T14:02:26.540758Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# -------------------------\n# 2. MINERAL TARGETING FEATURES\n# -------------------------\ndef create_mineral_features(df):\n    df['Ni/Cu_Ratio'] = df['Ni_ppm'] / (df['Cu_ppm'] + 1e-6)\n    df['PGE_Index'] = (df['Pt_ppb'] + df['Pd_ppb']) / 1000\n    df['LREE/HREE'] = (df['La_ppm'] + df['Ce_ppm']) / (df.get('Yb_ppm', 1) + 1e-6)\n    df['Argillic_Index'] = (df['Al2O3_%'] + df['K2O_%']) / (df['Si02_%'] + 1e-6)\n    df['Chlorite_Index'] = df['MgO_%'] / (df['Fe2O3_%'] + 1e-6)\n    df['Zr/Hf_Depth'] = df['Zr_ppm'] / (df['Hf_ppm'] + 1e-6)\n    print(\"âœ… Added 5 genetic model features\")\n    return df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T14:02:27.413349Z","iopub.execute_input":"2025-05-12T14:02:27.413634Z","iopub.status.idle":"2025-05-12T14:02:27.420215Z","shell.execute_reply.started":"2025-05-12T14:02:27.413616Z","shell.execute_reply":"2025-05-12T14:02:27.419126Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# -------------------------\n# 3. DATA SPLITTING (Multi-label)\n# -------------------------\ndef split_data(df, test_size=0.2, val_size=0.2):\n    target_elements = ['Cu_ppm', 'Ni_ppm', 'Co_ppm', 'Zn_ppm', 'La_ppm', 'Ce_ppm', 'Au_ppb', 'Pt_ppb']\n    for el in target_elements:\n        threshold = df[el].quantile(0.85)\n        df[f\"{el}_label\"] = (df[el] > threshold).astype(int)\n    target_cols = [f\"{el}_label\" for el in target_elements]\n    X = df.drop(columns=target_cols)\n    y = df[target_cols]\n    X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n    X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=val_size, random_state=42)\n    return X_train, X_val, X_test, y_train, y_val, y_test, target_cols","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T14:02:28.777870Z","iopub.execute_input":"2025-05-12T14:02:28.778185Z","iopub.status.idle":"2025-05-12T14:02:28.784895Z","shell.execute_reply.started":"2025-05-12T14:02:28.778165Z","shell.execute_reply":"2025-05-12T14:02:28.783885Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# -------------------------\n# 4. MINERAL PREDICTION MODEL (Updated with LayerNorm)\n# -------------------------\nclass MineralNN(nn.Module):\n    def __init__(self, input_dim):\n        super().__init__()\n        self.attention = nn.Sequential(\n            nn.Linear(input_dim, input_dim),\n            nn.Softmax(dim=1)\n        )\n        self.classifier = nn.Sequential(\n            nn.Linear(input_dim, 256),\n            nn.ReLU(),\n            nn.LayerNorm(256),  # Changed from BatchNorm to LayerNorm\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, 8),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        attn_weights = self.attention(x)\n        x = x * attn_weights\n        return self.classifier(x), attn_weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T14:02:30.753003Z","iopub.execute_input":"2025-05-12T14:02:30.753390Z","iopub.status.idle":"2025-05-12T14:02:30.760690Z","shell.execute_reply.started":"2025-05-12T14:02:30.753364Z","shell.execute_reply":"2025-05-12T14:02:30.759651Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"\n# -------------------------\n# 5. TRAINING FUNCTIONS (Updated with drop_last=True)\n# -------------------------\ndef train_model(X_train, y_train, X_val, y_val):\n    train_data = TensorDataset(torch.tensor(X_train.values, dtype=torch.float32), \n                             torch.tensor(y_train.values, dtype=torch.float32))\n    val_data = TensorDataset(torch.tensor(X_val.values, dtype=torch.float32), \n                           torch.tensor(y_val.values, dtype=torch.float32))\n    \n    # Added drop_last=True to prevent single-sample batches\n    train_loader = DataLoader(train_data, batch_size=32, shuffle=True, drop_last=True)\n    val_loader = DataLoader(val_data, batch_size=32, drop_last=True)\n    \n    model = MineralNN(X_train.shape[1])\n    criterion = nn.BCELoss()\n    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n    \n    best_val_loss = float('inf')\n    train_losses, val_losses = [], []\n    \n    for epoch in range(50):\n        model.train()\n        train_loss = 0\n        for batch_X, batch_y in train_loader:\n            optimizer.zero_grad()\n            pred, _ = model(batch_X)\n            loss = criterion(pred, batch_y)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n        \n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                pred, _ = model(batch_X)\n                val_loss += criterion(pred, batch_y).item()\n                \n        train_losses.append(train_loss/len(train_loader))\n        val_losses.append(val_loss/len(val_loader))\n        \n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), 'best_model.pth')\n            \n        print(f\"Epoch {epoch+1:02d}: Train Loss = {train_losses[-1]:.4f}, Val Loss = {val_losses[-1]:.4f}\")\n    \n    return model, train_losses, val_losses\n\ndef plot_training_history(train_losses, val_losses):\n    plt.figure(figsize=(8, 5))\n    plt.plot(train_losses, label='Train Loss')\n    plt.plot(val_losses, label='Validation Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Training History')\n    plt.legend()\n    plt.grid(True)\n    plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n    plt.close()\n\ndef generate_3d_maps(df, model, scaler, feature_names, target_cols):\n    X_raw = df[feature_names].copy()\n    X_scaled = scaler.transform(X_raw)\n    \n    with torch.no_grad():\n        probs, _ = model(torch.tensor(X_scaled, dtype=torch.float32))\n    \n    minerals = ['Cu_ppm', 'Ni_ppm', 'Co_ppm', 'Zn_ppm', 'La_ppm', 'Ce_ppm', 'Au_ppb', 'Pt_ppb']\n    for i, mineral in enumerate(minerals):\n        fig = go.Figure(data=[\n            go.Scatter3d(\n                x=df['X'], y=df['Y'], z=-df['Zr/Hf_Depth']*100,\n                mode='markers',\n                marker=dict(\n                    size=5,\n                    color=probs[:, i].numpy(),\n                    colorscale='Hot',\n                    cmin=0,\n                    cmax=1,\n                    opacity=0.8,\n                    colorbar=dict(title='Probability')\n                ),\n                text=df[mineral]\n            )\n        ])\n        fig.update_layout(\n            scene=dict(\n                xaxis_title='Longitude',\n                yaxis_title='Latitude',\n                zaxis_title='Depth Proxy (Zr/Hf)'\n            ),\n            title=f\"3D Prospectivity Map â€” {mineral}\"\n        )\n        fig.write_html(f\"3d_prospectivity_{mineral}.html\")\n\ndef plot_mineral_clusters(X, y, feature_names):\n    tsne = TSNE(n_components=2, random_state=42)\n    tsne_results = tsne.fit_transform(X)\n    \n    dominant_label = y.idxmax(axis=1)\n    fig = go.Figure()\n    \n    for label in dominant_label.unique():\n        mask = dominant_label == label\n        fig.add_trace(go.Scatter(\n            x=tsne_results[mask, 0],\n            y=tsne_results[mask, 1],\n            mode='markers',\n            name=label.replace('_label', ''),\n            marker=dict(size=5),\n            text=dominant_label[mask]\n        ))\n    \n    fig.update_layout(\n        title=\"Mineral Composition Clusters (t-SNE)\",\n        xaxis_title=\"t-SNE 1\",\n        yaxis_title=\"t-SNE 2\"\n    )\n    fig.write_html(\"mineral_clusters.html\")\n\ndef plot_confusion_matrices(y_true, y_pred, target_cols):\n    for i, col in enumerate(target_cols):\n        cm = confusion_matrix(y_true[:, i], y_pred[:, i])\n        plt.figure(figsize=(4, 3))\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n        plt.title(f\"Confusion Matrix: {col}\")\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"True\")\n        plt.tight_layout()\n        plt.savefig(f\"confusion_matrix_{col}.png\", dpi=300, bbox_inches='tight')\n        plt.close()\n\ndef generate_shap_analysis(model, X_train_scaled, feature_names, target_cols):\n    # Use a subset for faster computation\n    background = shap.utils.sample(X_train_scaled.values, 100)\n    explain_data = shap.utils.sample(X_train_scaled.values, 200)\n    \n    def model_wrapper(x):\n        x_tensor = torch.tensor(x, dtype=torch.float32)\n        with torch.no_grad():\n            preds, _ = model(x_tensor)\n        return preds.numpy()\n    \n    explainer = shap.Explainer(model_wrapper, background)\n    shap_values = explainer(explain_data)\n    \n    # Global SHAP summary for each target\n    for i, target in enumerate(target_cols):\n        plt.figure()\n        shap.summary_plot(shap_values[:, :, i].values, \n                         features=explain_data, \n                         feature_names=feature_names,\n                         show=False)\n        plt.title(f\"SHAP Summary - {target}\")\n        plt.tight_layout()\n        plt.savefig(f\"shap_summary_{target}.png\", dpi=300, bbox_inches='tight')\n        plt.close()\n    \n    # Force plots for first 3 samples\n    for i, target in enumerate(target_cols):\n        for j in range(3):\n            plt.figure()\n            shap.plots.force(shap_values[j, :, i], \n                           features=explain_data[j], \n                           feature_names=feature_names,\n                           matplotlib=True, show=False)\n            plt.title(f\"SHAP Force Plot - {target} (Sample {j+1})\")\n            plt.tight_layout()\n            plt.savefig(f\"shap_force_{target}_sample{j+1}.png\", dpi=300, bbox_inches='tight')\n            plt.close()\n    \n    # Comprehensive SHAP summary\n    plt.figure(figsize=(12, 8))\n    shap_values_all = np.stack([shap_values[:, :, i].values for i in range(len(target_cols))])\n    feature_importance = np.abs(shap_values_all).mean(axis=1)\n    norm_importance = feature_importance / feature_importance.sum(axis=1, keepdims=True)\n    \n    plot_df = pd.DataFrame(\n        norm_importance.T,\n        index=feature_names,\n        columns=target_cols\n    )\n    plot_df['total'] = plot_df.sum(axis=1)\n    plot_df = plot_df.sort_values('total', ascending=False).drop('total', axis=1)\n    \n    sns.heatmap(\n        plot_df,\n        cmap='viridis',\n        annot=True,\n        fmt='.2f',\n        cbar_kws={'label': 'Normalized SHAP Importance'},\n        linewidths=0.5\n    )\n    plt.title('Comprehensive SHAP Feature Importance')\n    plt.xlabel('Mineral Targets')\n    plt.ylabel('Geochemical Features')\n    plt.tight_layout()\n    plt.savefig('comprehensive_shap_summary.png', dpi=300, bbox_inches='tight')\n    plt.close()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T14:02:34.133574Z","iopub.execute_input":"2025-05-12T14:02:34.134562Z","iopub.status.idle":"2025-05-12T14:02:34.160432Z","shell.execute_reply.started":"2025-05-12T14:02:34.134514Z","shell.execute_reply":"2025-05-12T14:02:34.159550Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# -------------------------\n# MAIN WORKFLOW\n# -------------------------\nif __name__ == \"__main__\":\n    # 1. Load and clean data\n    df, pt = load_and_clean(\"/kaggle/input/my-dataset/NGCM-Stream-Sediment-Analysis-Updated.xlsx\")\n    df = create_mineral_features(df)\n    \n    # 2. Split data\n    X_train, X_val, X_test, y_train, y_val, y_test, target_cols = split_data(df)\n    \n    # 3. Scale data\n    scaler = StandardScaler().fit(X_train.select_dtypes(include=np.number))\n    X_train_scaled = X_train.copy()\n    X_val_scaled = X_val.copy()\n    X_test_scaled = X_test.copy()\n    X_train_scaled[X_train.select_dtypes(include=np.number).columns] = scaler.transform(X_train.select_dtypes(include=np.number))\n    X_val_scaled[X_val.select_dtypes(include=np.number).columns] = scaler.transform(X_val.select_dtypes(include=np.number))\n    X_test_scaled[X_test.select_dtypes(include=np.number).columns] = scaler.transform(X_test.select_dtypes(include=np.number))\n    feature_names = X_train.select_dtypes(include=np.number).columns.tolist()\n    \n    # 4. Train model\n    model, train_losses, val_losses = train_model(X_train_scaled, y_train, X_val_scaled, y_val)\n    plot_training_history(train_losses, val_losses)\n    \n    # 5. Evaluate model\n    with torch.no_grad():\n        # Validation metrics\n        val_probs, _ = model(torch.tensor(X_val_scaled.values, dtype=torch.float32))\n        val_pred = (val_probs > 0.5).float().numpy()\n        val_true = y_val.values\n        val_acc = (val_pred == val_true).mean()\n        print(f\"\\nðŸ“ˆ Validation Set Accuracy (per label mean): {val_acc:.4f}\")\n        \n        # Test metrics\n        test_probs, _ = model(torch.tensor(X_test_scaled.values, dtype=torch.float32))\n        test_pred = (test_probs > 0.5).float().numpy()\n        test_true = y_test.values\n        test_acc = (test_pred == test_true).mean()\n        print(f\"ðŸ“ˆ Test Set Accuracy (per label mean): {test_acc:.4f}\")\n    \n    # 6. Print classification report\n    print(\"\\nðŸ“Š Multi-label Test Set Performance:\")\n    print(classification_report(y_test, test_pred, target_names=target_cols))\n    \n    # 7. Print AUC-ROC scores\n    print(\"AUC-ROC Scores:\")\n    for i, col in enumerate(target_cols):\n        try:\n            auc_score = roc_auc_score(y_test[col], test_probs.numpy()[:, i])\n            print(f\"{col}: {auc_score:.3f}\")\n        except ValueError:\n            print(f\"{col}: Not computable (only one class present)\")\n    \n    # 8. Generate visualizations\n    generate_3d_maps(df, model, scaler, feature_names, target_cols)\n    plot_mineral_clusters(X_test_scaled.values, y_test, feature_names)\n    plot_confusion_matrices(test_true, test_pred, target_cols)\n    generate_shap_analysis(model, X_train_scaled, feature_names, target_cols)\n    \n    # 9. Save model parameters\n    params = {\n        \"data_preprocessing\": {\n            \"initial_samples\": df.shape[0],\n            \"final_features\": len(feature_names),\n            \"outlier_method\": \"IsolationForest (5%)\",\n            \"transformations\": [\"PowerTransformer\", \"StandardScaler\"]\n        },\n        \"model\": {\n            \"architecture\": \"AttentionNN(256-128-8)\",\n            \"optimizer\": \"AdamW (lr=0.001, wd=0.01)\",\n            \"training_samples\": len(X_train),\n            \"validation_accuracy\": float(val_acc),\n            \"test_accuracy\": float(test_acc),\n            \"output_targets\": target_cols\n        }\n    }\n    with open(\"model_params.json\", \"w\") as f:\n        json.dump(params, f, indent=4)\n    \n    print(\"\\nâœ… All requested outputs generated:\")\n    print(\"- 3d_prospectivity_<mineral>.html (8 files)\")\n    print(\"- mineral_clusters.html\")\n    print(\"- training_history.png\")\n    print(\"- confusion_matrix_<label>.png (8 files)\")\n    print(\"- comprehensive_shap_summary.png\")\n    print(\"- model_params.json (now includes accuracy metrics)\")\n    print(\"- best_model.pth\")\n    print(\"- shap_summary_<target>.png (8 files)\")\n    print(\"- shap_force_<target>_sampleX.png (24 files)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T14:02:42.734812Z","iopub.execute_input":"2025-05-12T14:02:42.735164Z","iopub.status.idle":"2025-05-12T14:04:52.120675Z","shell.execute_reply.started":"2025-05-12T14:02:42.735141Z","shell.execute_reply":"2025-05-12T14:04:52.119759Z"}},"outputs":[{"name":"stdout","text":"âœ… Cleaned Data: 9503 samples, 21 features\nâœ… Added 5 genetic model features\nEpoch 01: Train Loss = 0.2196, Val Loss = 0.1204\nEpoch 02: Train Loss = 0.0928, Val Loss = 0.0707\nEpoch 03: Train Loss = 0.0620, Val Loss = 0.0513\nEpoch 04: Train Loss = 0.0475, Val Loss = 0.0452\nEpoch 05: Train Loss = 0.0397, Val Loss = 0.0392\nEpoch 06: Train Loss = 0.0369, Val Loss = 0.0386\nEpoch 07: Train Loss = 0.0312, Val Loss = 0.0297\nEpoch 08: Train Loss = 0.0300, Val Loss = 0.0305\nEpoch 09: Train Loss = 0.0269, Val Loss = 0.0279\nEpoch 10: Train Loss = 0.0234, Val Loss = 0.0278\nEpoch 11: Train Loss = 0.0253, Val Loss = 0.0245\nEpoch 12: Train Loss = 0.0206, Val Loss = 0.0230\nEpoch 13: Train Loss = 0.0203, Val Loss = 0.0249\nEpoch 14: Train Loss = 0.0198, Val Loss = 0.0352\nEpoch 15: Train Loss = 0.0190, Val Loss = 0.0207\nEpoch 16: Train Loss = 0.0178, Val Loss = 0.0270\nEpoch 17: Train Loss = 0.0168, Val Loss = 0.0208\nEpoch 18: Train Loss = 0.0181, Val Loss = 0.0245\nEpoch 19: Train Loss = 0.0163, Val Loss = 0.0217\nEpoch 20: Train Loss = 0.0158, Val Loss = 0.0189\nEpoch 21: Train Loss = 0.0143, Val Loss = 0.0284\nEpoch 22: Train Loss = 0.0163, Val Loss = 0.0189\nEpoch 23: Train Loss = 0.0122, Val Loss = 0.0248\nEpoch 24: Train Loss = 0.0171, Val Loss = 0.0200\nEpoch 25: Train Loss = 0.0146, Val Loss = 0.0223\nEpoch 26: Train Loss = 0.0163, Val Loss = 0.0189\nEpoch 27: Train Loss = 0.0120, Val Loss = 0.0186\nEpoch 28: Train Loss = 0.0112, Val Loss = 0.0185\nEpoch 29: Train Loss = 0.0103, Val Loss = 0.0254\nEpoch 30: Train Loss = 0.0155, Val Loss = 0.0245\nEpoch 31: Train Loss = 0.0106, Val Loss = 0.0186\nEpoch 32: Train Loss = 0.0119, Val Loss = 0.0230\nEpoch 33: Train Loss = 0.0104, Val Loss = 0.0247\nEpoch 34: Train Loss = 0.0103, Val Loss = 0.0262\nEpoch 35: Train Loss = 0.0130, Val Loss = 0.0244\nEpoch 36: Train Loss = 0.0117, Val Loss = 0.0179\nEpoch 37: Train Loss = 0.0119, Val Loss = 0.0330\nEpoch 38: Train Loss = 0.0091, Val Loss = 0.0174\nEpoch 39: Train Loss = 0.0100, Val Loss = 0.0229\nEpoch 40: Train Loss = 0.0106, Val Loss = 0.0262\nEpoch 41: Train Loss = 0.0088, Val Loss = 0.0198\nEpoch 42: Train Loss = 0.0077, Val Loss = 0.0276\nEpoch 43: Train Loss = 0.0092, Val Loss = 0.0242\nEpoch 44: Train Loss = 0.0096, Val Loss = 0.0262\nEpoch 45: Train Loss = 0.0121, Val Loss = 0.0210\nEpoch 46: Train Loss = 0.0075, Val Loss = 0.0226\nEpoch 47: Train Loss = 0.0105, Val Loss = 0.0237\nEpoch 48: Train Loss = 0.0110, Val Loss = 0.0312\nEpoch 49: Train Loss = 0.0081, Val Loss = 0.0251\nEpoch 50: Train Loss = 0.0169, Val Loss = 0.0265\n\nðŸ“ˆ Validation Set Accuracy (per label mean): 0.9908\nðŸ“ˆ Test Set Accuracy (per label mean): 0.9885\n\nðŸ“Š Multi-label Test Set Performance:\n              precision    recall  f1-score   support\n\nCu_ppm_label       0.93      0.95      0.94       275\nNi_ppm_label       0.97      0.95      0.96       292\nCo_ppm_label       0.97      0.92      0.94       278\nZn_ppm_label       0.93      0.99      0.96       257\nLa_ppm_label       0.98      0.95      0.96       274\nCe_ppm_label       1.00      0.93      0.96       271\nAu_ppb_label       0.98      0.99      0.99       286\nPt_ppb_label       0.95      1.00      0.97       284\n\n   micro avg       0.96      0.96      0.96      2217\n   macro avg       0.96      0.96      0.96      2217\nweighted avg       0.96      0.96      0.96      2217\n samples avg       0.58      0.58      0.58      2217\n\nAUC-ROC Scores:\nCu_ppm_label: 0.998\nNi_ppm_label: 0.999\nCo_ppm_label: 0.999\nZn_ppm_label: 0.999\nLa_ppm_label: 0.999\nCe_ppm_label: 0.999\nAu_ppb_label: 0.998\nPt_ppb_label: 1.000\n","output_type":"stream"},{"name":"stderr","text":"PermutationExplainer explainer: 201it [00:22,  6.33it/s]                         \n","output_type":"stream"},{"name":"stdout","text":"\nâœ… All requested outputs generated:\n- 3d_prospectivity_<mineral>.html (8 files)\n- mineral_clusters.html\n- training_history.png\n- confusion_matrix_<label>.png (8 files)\n- comprehensive_shap_summary.png\n- model_params.json (now includes accuracy metrics)\n- best_model.pth\n- shap_summary_<target>.png (8 files)\n- shap_force_<target>_sampleX.png (24 files)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 0 Axes>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 0 Axes>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 0 Axes>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 0 Axes>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 0 Axes>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 0 Axes>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 0 Axes>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 0 Axes>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 0 Axes>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 0 Axes>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 0 Axes>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 0 Axes>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 0 Axes>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 0 Axes>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 0 Axes>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 0 Axes>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 0 Axes>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 0 Axes>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 0 Axes>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 0 Axes>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 0 Axes>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 0 Axes>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 0 Axes>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 0 Axes>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}